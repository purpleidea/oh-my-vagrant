# -*- mode: ruby -*-
# vi: set ft=ruby noet :

# This is: Oh My Vagrant!
# Copyright (C) 2012-2015+ James Shubin
# Written by James Shubin <james@shubin.ca>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = '2'

require 'ipaddr'
require 'yaml'
require 'erb'
require 'ostruct'
require 'base64'

#
#	globals
#
# thanks to the gluster.org community for the public box hosting
default_boxurlprefix = 'https://download.gluster.org/pub/gluster/purpleidea/vagrant'
really_use_rm = false	# this causes the rm -rf to actually happen if set true

#
#	vms
#
# NOTE: you can specify the list of vms here, or in the omv.yaml file...
# NOTE: if you set the vagrant vms array to nil, you'll default to this list
vms = [
#	{:name => 'example1', :docker => true, :puppet => true, },	# example1
#	{:name => 'example2', :docker => ['centos', 'fedora'], },	# example2
#	{:name => 'example3', :docker =>
#		[
#			{:name => 'centos', },
#			{:name => 'fedora', },
#		],
#	},								# example3
#	{:name => 'example4', :image => 'centos-6', :puppet => true, },	# example4
#	{:name => 'example5', :image => 'rhel-7.0', :poolid => true, },	# example5
#	{:name => 'example6', :puppet => true, :classes =>		# example6
#		['module1', 'module2', 'module3'],
#	},
#	{:name => 'example7', :puppet => true, :classes =>		# example7
#		{
#			'module1' => {'arg1' => 'val1', 'arg2' => 'val2'},
#			'module2' => nil,
#		},
#	},
#	{:name => 'example8', :puppet => true, :ip => '192.168.123.3'},
]

# mutable by ARGV and settings file
domain = 'example.com'		# demain domain to use (yes this *can* work)
network = '192.168.123.0/24'	# default network to use
image = 'centos-7.1'		# default image name
boxurlprefix = ''		# default url prefix (useful for private boxes)
sync = 'rsync'			# default sync type
folder = ''			# default folder prefix
extern = []			# default external module definitions
puppet = false			# default use of puppet or not
classes = []			# default list or hash of classes to include
docker = false			# default use of docker or not
kubernetes = false		# default use of kubernetes or not
ansible = []			# default ansible group list
playbook = []			# default ansible playbook
cachier = false			# default cachier usage
#vms = []			# default list of vms to build (defaults above)
namespace = 'omv'		# default namespace (also used as network name)
count = 1			# default number of hosts to build
# TODO: subscription manager needs to grow a way to generate an 'API key' that
# can be used instead of a password so that it can be safely used in scripts...
username = ''			# default subscription manager username
password = ''			# default subscription manager password
poolid = true			# default list of poolid's (true to auto-attach)
repos = []			# default list of extra repos's to enable
update = false			# determine whether or not to update the box on vagrant up

def array_values_to_array_of_hashes(l)
	result = l
	if l.is_a?(Array) and l.length > 0
		if not l[0].is_a?(Hash)	# check the first value
			result = l.each_with_object([]) { |x, a| a.push({:name => x}) }
		end
	end
	return result
end

# useful for unindenting heredoc's
class String
	def unindent
		gsub(/^#{scan(/^\s*/).min_by{|l|l.length}}/, '')
	end
end
# eg:
#test = <<-EOT.unindent
#	42
#EOT

projectdir = File.expand_path File.dirname(__FILE__)	# vagrant project dir!!

#
#	~/.config/oh-my-vagrant/ parsing
#
# Vagrant is using it's own gem path. Using local xdg gem
# Another option could be adding xdg gem directly to vagrant
begin
	gemlibdir = File.join(projectdir, 'gems/xdg/lib')
	$:.unshift(gemlibdir)	# add local path for gems
	require 'xdg'
	auth_config = File.join("#{XDG['CONFIG']}/oh-my-vagrant/", 'auth.yaml')

rescue LoadError
	$stderr.puts "Warning: Can't to load 'xdg' gem - check local gem path"
	$stderr.puts gemlibdir
	auth_config = File.join("#{Dir.home}/.oh-my-vagrant/", 'auth.yaml')
	if not File.exist?(auth_config)
		auth_config = File.join("#{Dir.home}/.oh-my-vagrant.yaml")
	end
end
if File.exist?(auth_config)
	settings = YAML::load_file auth_config
	global_username = settings[:username]
	global_password = settings[:password]
end

#
#	ARGV parsing
#
f = g = File.join(projectdir, 'omv.yaml')
# check for omv.yaml in folder subdirectory
load_folder = false
if File.exist?(f)
	settings = YAML::load_file f
	folder = settings[:folder]

	(0..ARGV.length-1).each do |i|
		if ARGV[i].start_with?(arg='--vagrant-folder=')
			v = ARGV.delete_at(i).dup
			v.slice! arg
			folder = v.to_s		# set folder prefix
			load_folder = true
			break
		end
	end

	# if folder is '', then this doesn't do any different from the original
	ff = File.join(projectdir, folder, 'omv.yaml')
	if File.exist?(ff)
		f = ff
	end
end

# load settings
if File.exist?(f)
	settings = YAML::load_file f
	domain = settings[:domain]
	network = settings[:network]
	image = settings[:image]
	boxurlprefix = settings[:boxurlprefix]
	sync = settings[:sync]
	folder = settings[:folder] if not(load_folder)
	extern = settings[:extern]
	puppet = settings[:puppet]
	classes = settings[:classes]
	docker = settings[:docker]
	kubernetes = settings [:kubernetes]
	ansible = settings[:ansible]
	playbook = settings[:playbook]
	cachier = settings[:cachier]
	if settings[:vms].is_a?(Array)
		vms = settings[:vms]
	end
	namespace = settings[:namespace]
	count = settings[:count]
	username = settings[:username]
	password = settings[:password]
	poolid = settings[:poolid]
	repos = settings[:repos]
	update = settings[:update]
	really_use_rm = settings[:reallyrm]
end

# ARGV parser
skip = 0
while skip < ARGV.length
	#puts "#{skip}, #{ARGV[skip]}"	# debug
	if ARGV[skip].start_with?(arg='--vagrant-domain=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg
		#puts "#{arg}, #{v}"	# debug

		domain = v.to_s		# set domain

	elsif ARGV[skip].start_with?(arg='--vagrant-network=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		network = v.to_s	# set network range

	elsif ARGV[skip].start_with?(arg='--vagrant-image=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		image = v.to_s	# set base image

	elsif ARGV[skip].start_with?(arg='--vagrant-boxurlprefix=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		boxurlprefix = v.to_s	# set box url prefix

	elsif ARGV[skip].start_with?(arg='--vagrant-sync=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		sync = v.to_s		# set sync type

	elsif ARGV[skip].start_with?(arg='--vagrant-folder=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		folder = v.to_s		# set folder prefix

	elsif ARGV[skip].start_with?(arg='--vagrant-extern=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.include? ',' and v.split(',').length > 0
			extern = v.split(',')
			# FIXME: further parse the chunks into hashes...
		else
			extern = []
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-puppet=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		puppet = v.to_s		# set puppet flag
		if ['true', 'yes'].include?(puppet.downcase)
			puppet = true
		else
			puppet = false
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-puppet-classes=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.include? ',' and v.split(',').length > 0
			classes = v.split(',')
		else
			classes = []
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-docker=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.include? ',' and v.split(',').length > 0
			v = v.split(',')
			docker = array_values_to_array_of_hashes(v)
		else
			docker = v.to_s		# set docker flag
			if ['true', 'yes'].include?(docker.downcase)
				docker = true
			else
				docker = false
			end
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-kubernetes=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		kubernetes = v.to_s		# set kubernetes flag
		if ['true', 'yes'].include?(kubernetes.downcase)
			kubernetes = true
		else
			kubernetes = false
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-ansible=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.split(',').length > 0
			ansible = v.split(',')
		else
			ansible = []
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-playbook=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.split(',').length > 0
			playbook = v.split(',')
		else
			playbook = []
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-cachier=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		cachier = v.to_s	# set cachier flag
		if ['true', 'yes'].include?(cachier.downcase)
			cachier = true
		else
			cachier = false
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-vms=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.include? ',' and v.split(',').length > 0
			v = v.split(',')
			vms = array_values_to_array_of_hashes(v)
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-namespace=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		namespace = v.to_s	# set namespace

	elsif ARGV[skip].start_with?(arg='--vagrant-count=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		count = v.to_i		# set host count

	elsif ARGV[skip].start_with?(arg='--vagrant-username=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		username = v.to_s	# set username

	elsif ARGV[skip].start_with?(arg='--vagrant-password=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		password = v.to_s	# set password

	elsif ARGV[skip].start_with?(arg='--vagrant-poolid=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.include? ',' and v.split(',').length > 0
			poolid = v.split(',')
		else
			poolid = v.to_s		# set poolid boolean flag
			if ['true', 'auto'].include?(poolid.downcase)
				poolid = true
			else
				poolid = false
			end
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-repos=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		if v.is_a?(String) and v.include? ',' and v.split(',').length > 0
			repos = v.split(',')
		else
			repos = v.to_s		# set repos boolean flag
			if ['true', 'all'].include?(repos.downcase)
				repos = true
			else
				repos = false
			end
		end

	elsif ARGV[skip].start_with?(arg='--vagrant-reallyrm=')
		v = ARGV.delete_at(skip).dup
		v.slice! arg

		really_use_rm = v.to_s
		if ['true'].include?(really_use_rm.downcase)
			really_use_rm = true
		else
			really_use_rm = false
		end

	else	# skip over "official" vagrant args
		skip = skip + 1
	end
end

# save settings (ARGV overrides)
settings = {
	:domain => domain,
	:network => network,
	:image => image,
	:boxurlprefix => boxurlprefix,
	:sync => sync,
	:folder => folder,
	:extern => extern,
	:puppet => puppet,
	:classes => classes,
	:docker => docker,
	:kubernetes => kubernetes,
	:ansible => ansible,
	:playbook => playbook,
	:cachier => cachier,
	:vms => vms,
	:namespace => namespace,
	:count => count,
	:username => username,
	:password => password,
	:poolid => poolid,
	:repos => repos,
	:update => update,
	:reallyrm => really_use_rm,
}
File.open(f, 'w') do |file|
	file.write settings.to_yaml
end
File.open(g, 'w') do |file|
	file.write settings.to_yaml
end

username = global_username if "#{username}" == ''
password = global_password if "#{password}" == ''
username = '' if username.nil?
password = '' if password.nil?
#puts "ARGV: #{ARGV}"	# debug

# networking
network_obj = IPAddr.new network
range = network_obj.to_range.to_a
cidr = (32-(Math.log(range.length)/Math.log(2))).to_i
offset = 100		# start hosts after here
# remove reserved values
range[0] = '__reserved_network_addr'	# network
range[1] = '__reserved_router_addr'	# router (reserved)
#puts range[2].to_s	# puppetmaster
#puts range[3].to_s	# vip

# prepend $count vms onto the vms list...
extra = []
(1..count).each do |i|
	h = "#{namespace}#{i}"
	# generate names and add in the defaults
	extra.push({:name => h, :docker => docker})
end
vms = extra.concat vms

# add in puppet host if requested
if puppet
	extra = [{:name => 'puppet', :ip => range[2].to_s}]
	range[2] = '__reserved_puppet_addr'
	vms = extra.concat vms

	if not(classes.is_a?(Hash)) and not(classes.is_a?(Array))
		classes = []
	end
end

# transform docker value if necessary
docker = array_values_to_array_of_hashes(docker)

# erase host information from puppet so that the user can do partial rebuilds
snoop = ARGV.select { |x| !x.start_with?('-') }
if snoop.length > 1 and snoop[0] == 'destroy'
	snoop.shift	# left over array snoop should be list of hosts
	if snoop.include?('puppet')	# doesn't matter then...
		snoop = []
	end
else
	# important! clear snoop because we're not using 'destroy'
	snoop = []
end

# figure out which hosts are getting destroyed
destroy = ARGV.select { |x| !x.start_with?('-') }
if destroy.length > 0 and destroy[0] == 'destroy'
	destroy.shift	# left over array destroy should be list of hosts or []
	if destroy.length == 0
		destroy = true	# destroy everything
	end
else
	destroy = false		# destroy nothing
end

# figure out which hosts are getting provisioned
provision = ARGV.select { |x| !x.start_with?('-') }
if provision.length > 0 and ['up', 'provision'].include?(provision[0])
	provision.shift	# left over array provision should be list of hosts or []
	if provision.length == 0
		provision = true	# provision everything
	end
else
	provision = false		# provision nothing
end

# XXX: workaround for: https://github.com/mitchellh/vagrant/issues/2447
# only run on 'vagrant init' or if it's the first time running vagrant
if sync == 'nfs' and ((ARGV.length > 0 and ARGV[0] == 'init') or not(File.exist?(f)))
	`sudo systemctl restart nfs-server`
	`firewall-cmd --permanent --zone public --add-service mountd`
	`firewall-cmd --permanent --zone public --add-service rpc-bind`
	`firewall-cmd --permanent --zone public --add-service nfs`
	`firewall-cmd --reload`
end

# this extern code should be one of the *last* things before the big configure!
folder = (folder + '/') if not(folder.end_with?('/'))	# ensure trailing slash
folder = '' if folder.start_with?('/')	# only relative paths are allowed...

if load_folder
	puppet_basedir = File.join(projectdir, folder, 'puppet/', 'modules/')
	# NOTE: i called the ansible child dir 'modules' because i didn't know
	# what to call it. Suggestions welcome! It's not really 'playbooks' or
	# 'roles' really, so it's 'modules' until something better comes along
	ansible_basedir = File.join(projectdir, folder, 'ansible/', 'modules/')
	docker_basedir = File.join(projectdir, folder, 'docker/')
	kubernetes_basedir = File.join(projectdir, folder, 'kubernetes/', 'applications/')
	ktemplates_basedir = File.join(projectdir, folder, 'kubernetes/', 'templates/')
else
	puppet_basedir = File.join(projectdir, 'puppet/', 'modules/')
	ansible_basedir = File.join(projectdir, 'ansible/', 'modules/')
	docker_basedir = File.join(projectdir, 'docker/')
	kubernetes_basedir = File.join(projectdir, 'kubernetes/', 'applications/')
	ktemplates_basedir = File.join(projectdir, 'kubernetes/', 'templates/')
end

native = []	# native files
native += `cd "#{puppet_basedir}" && git ls-files`.strip.split("\n")
native += `cd "#{ansible_basedir}" && git ls-files`.strip.split("\n")
native += `cd "#{docker_basedir}" && git ls-files`.strip.split("\n")
native += `cd "#{kubernetes_basedir}" && git ls-files`.strip.split("\n")
native += `cd "#{ktemplates_basedir}" && git ls-files`.strip.split("\n")

if extern.length > 0
	extern.each do |i|
		t = i.fetch('type', nil)
		s = i.fetch('system', nil)	# puppet, ansible, and so on...
		if s == 'puppet'
			basedir = puppet_basedir
		elsif s == 'ansible'
			basedir = ansible_basedir
		elsif s == 'docker'
			basedir = docker_basedir
		elsif s == 'kubernetes'
			basedir = kubernetes_basedir
		elsif s == 'ktemplates'
			basedir = ktemplates_basedir
		else
			$stderr.puts "Can't process extern system: '#{s}'."
			next
		end
		if t == 'git'
			repository = i.fetch('repository', nil)
			if repository.start_with?('~/')
				repository = File.expand_path(repository)
			end
			directory = i.fetch('directory', nil)
			branch = i.fetch('branch', nil)
			if repository.nil? or directory.nil?
				next
			end
			repo_okay = false
			bd = File.join("#{basedir}", "#{directory}")
			if File.exists?(bd)
				r = `cd "#{bd}" && git remote show -n origin | grep 'Fetch URL: ' | awk -F 'Fetch URL: ' '{print $2}'`.strip
				if r == repository
					repo_okay = true
				else
					$stderr.puts "Repository: '#{repository}' doesn't match installed."
					if really_use_rm
						`rm -rf "#{bd}"`
					else
						$stderr.puts `echo please: rm -rf "#{bd}"`	# safer for now
					end
				end
				# pull latest in if it exists
				r = `cd "#{bd}" && [ $(git fetch --dry-run 2>&1 | wc -l) -gt 0 ] || echo already up-to-date`.strip
				if r == ''
					r = `cd "#{bd}" && git checkout master && git pull`.strip
					$stderr.puts r
				end

			else
				r = `cd "#{basedir}" && git clone --recursive #{repository} #{directory}`
				$stderr.puts r
				repo_okay = true
			end
			if repo_okay and not branch.nil?
				r = `cd "#{bd}" && git checkout "#{branch}"`.strip
			end
			native.push("#{directory}")
		else
			$stderr.puts "Can't process extern type: '#{t}'."
		end
	end
end

# clean up any directories or files that shouldn't be present
(
	Dir.glob(puppet_basedir+'*') +
	Dir.glob(ansible_basedir+'*') +
	Dir.glob(docker_basedir+'*') +
	Dir.glob(kubernetes_basedir+'*') +
	Dir.glob(ktemplates_basedir+'*') +
[]).uniq.each do |i|
	b = File.basename(i)
	if not native.include?(b)
		if really_use_rm
			`rm -rf "#{i}"`
		else
			$stderr.puts `echo please: rm -rf "#{i}"`	# safer for now
		end
	end
end

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

	# workaround vagrant 1.7.2 insecure key change
	config.ssh.insert_key = false

	#config.landrush.enable	# TODO ?

	#
	#	cache
	#
	# NOTE: you should probably erase the cache between rebuilds if you are
	# installing older package versions. This is because the newer packages
	# will get cached, and then subsequently might get silently installed!!
	if cachier
		# TODO: this doesn't cache metadata, full offline operation not possible
		config.cache.auto_detect = true
		config.cache.enable :yum
		#config.cache.enable :apt
		if not ARGV.include?('--no-parallel')	# when running in parallel,
			config.cache.scope = :machine	# use the per machine cache
		end
		if sync == 'nfs'	# TODO: support other sync types here...
			config.cache.enable_nfs = true	# sets nfs => true on the synced_folder
			# the nolock option is required, otherwise the NFSv3 client will try to
			# access the NLM sideband protocol to lock files needed for /var/cache/
			# all of this can be avoided by using NFSv4 everywhere. die NFSv3, die!
			config.cache.mount_options = ['rw', 'vers=3', 'tcp', 'nolock']
		end
	end

	#
	#	vip
	#
	vip_ip = range[3].to_s
	vip_hostname = namespace
	range[3] = '__reserved_vip_addr'

	#
	#	hostmanager
	#
	# TODO: does this plugin still mess up the real vagrant-libvirt hostname ?
	config.hostmanager.enabled = true
	config.hostmanager.manage_host = false	# don't manage local /etc/hosts
	config.hostmanager.ignore_private_ip = false
	config.hostmanager.include_offline = true	# manage all the hosts!
	config.hostmanager.fqdn_friendly = true		# fqdns need to work...
	config.hostmanager.domain_name = domain		# use this domain name!
	config.hostmanager.extra_hosts = [
		{
			:host => "#{vip_hostname}.#{domain}",
			:ip => "#{vip_ip}",
			:aliases => ["#{vip_hostname}"],
		}
	]

	#
	#	set vms ips and do other pre-processing
	#
	ansible_groups = {}
	kubernetes_master = false	# TODO: in future allow an array of masters ?
	kubernetes_hosts = []	# list of every participating kubernetes host
	kubernetes_templates = ''
	kubernetes_json = []
	vms.each_with_index do |x, i|
		h = x[:name]
		if not x.fetch(:ip, nil) # if vm ip was not set in omv.yaml...
			x[:ip] = range[offset+i].to_s # ...generate it, and set vm ip
			#vms[i][:ip] = range[offset+i].to_s	# equivalent!
		end

		# kubernetes
		vm_kubernetes = x.fetch(:kubernetes, kubernetes)	# get value
		if kubernetes.is_a?(Hash) and vm_kubernetes.is_a?(Hash)
			vm_kubernetes = kubernetes.merge(vm_kubernetes)	# inherit from global
		end
		if vm_kubernetes
			kubernetes_hosts.push(h)
		end

		# build ansible_groups variable now, because needed in big loop
		vm_ansible = x.fetch(:ansible, ansible)	# get value
		ansible_groups[h] = vm_ansible if vm_ansible != []
	end
	if kubernetes_hosts.length > 0
		kubernetes_master = kubernetes_hosts[0]	# default to first host
		if kubernetes.is_a?(Hash)
			# a master key can set which kubernetes host is the master one!
			tmp = kubernetes.fetch('master', nil)
			if kubernetes_hosts.include?(tmp)
				kubernetes_master = tmp
			end

			# kubernetes templates base path inside repo, eg: v0.9.0
			tmp = kubernetes.fetch('ktemplates', 'default/latest/')	# pick!
			if tmp.is_a?(String)
				tmp = File.join(ktemplates_basedir, tmp)
				if File.exists?(tmp)
					kubernetes_templates = tmp
				end
			end

			# json definition file
			kubernetes_json = kubernetes.fetch('applications', [])	# pick!
			if not kubernetes_json.is_a?(Array)
				kubernetes_json = []
			end
		end
	end
	#$stderr.puts "Kubernetes master: #{kubernetes_master}"
	#$stderr.puts "Kubernetes others: #{kubernetes_hosts.join(',')}"

	#
	#	vms mainloop to define all the machines
	#
	vms.each_with_index do |x, i|
		h = x[:name]
		ip = x.fetch(:ip) # get value
		vm_image = x.fetch(:image, image)	# get value
		vm_docker = x.fetch(:docker, docker)	# get value
		vm_docker = array_values_to_array_of_hashes(vm_docker)
		vm_puppet = x.fetch(:puppet, puppet)	# get value
		vm_classes = x.fetch(:classes, classes)
		vm_poolid = x.fetch(:poolid, poolid)	# get value
		vm_repos = x.fetch(:repos, repos)	# get value
		vm_playbook = x.fetch(:playbook, playbook)	# get value
		vm_sync = x.fetch(:sync, sync)	# get value
		vm_update = x.fetch(:update, update) # get value

		# sanity check the user-supplied IP addresses and gracefully fail
		# puppet server is special, so we skip it here
		if h != 'puppet' then
			error_check = range.include? IPAddr.new ip
			message = "#{namespace}:#{h}: ERROR: #{ip} is not in the valid range of IPs: #{range[3].to_s}..#{range[-2].to_s}"
			message += "\nConsider changing 'network:' parameter in omv.yaml, or use the correct IP address from the range above!"
			raise Vagrant::Errors::VagrantError.new, message unless error_check
		end

		config.vm.define h.to_sym do |vm|
			vm.vm.hostname = h
			#vm.hostmanager.aliases = ["#{h}.#{domain}"]

			# this is the real network that we'll use...
			vm.vm.network :private_network,
			:ip => ip,
			:libvirt__dhcp_enabled => false,
			:libvirt__network_name => "#{namespace}",
			:virtualbox__intnet => "#{namespace}"

			#
			#	box (pre-built base image)
			#
			vm.vm.box = vm_image	# set vm specific image

			synced_folder = '/vagrant'	# default sync folder

			# use a specialized vagrant box if it exists :)
			if vm_docker
				if `vagrant box list | grep -q '^#{vm_image}-docker' && echo -n found` != ''
					vm.vm.box = "#{vm_image}-docker"
				end
			end

			# sync type
			disabled = false
			if vm.vm.box.start_with? 'atomic-' or vm_sync == ''
				disabled = true
			end
			vm.vm.synced_folder './', '/vagrant', disabled: disabled, type: vm_sync	# nfs, rsync

			if vm.vm.box.start_with? 'atomic-'
				synced_folder = '/home/vagrant/sync'
				if vm_sync != ''
					vm.vm.synced_folder '.', synced_folder, type: sync
				end
			end

			# box source url's
			# FIXME: boxes should be GPG signed and verified on dl!
			if "#{boxurlprefix}" != ''
				vm.vm.box_url = "#{boxurlprefix}#{vm.vm.box}/#{vm.vm.box}.box"
			end

			# FIXME: speed up or cache (memoize) this check...
			if `vagrant box list | grep -q '^#{vm.vm.box}' && echo -n found` != ''
				exists = true
			else
				if vm.vm.box_url.is_a?(String)
					puts "Running wget check on: '#{vm.vm.box_url}'"
					exists = `wget -q --spider #{vm.vm.box_url}`
				else
					exists = false
				end
			end
			if not(exists) and not (vm.vm.box.start_with? 'rhel-' or vm.vm.box.start_with? 'atomic-rhel-')
				vm.vm.box_url = "#{default_boxurlprefix}/#{vm.vm.box}/#{vm.vm.box}.box"
			end

			#vm.landrush.host h, ip	# TODO ?

			fv = File.join(projectdir, '.vagrant', "#{h}-hosts.done")
			if destroy.is_a?(TrueClass) or (destroy.is_a?(Array) and destroy.include?(h))
				if File.exists?(fv)	# safety
					puts "Unlocking shell provisioning for: #{h}..."
					File.delete(fv)	# delete hosts token
				end
			end

			# should we clean this puppet client machine?
			if puppet and vm_puppet and h != 'puppet' and snoop.include?(h)
				cmd = "puppet cert clean #{h}.#{domain}"
				puts "Running 'puppet cert clean' for: #{h}..."
				`vagrant ssh puppet -c 'sudo #{cmd}'`
				cmd = "puppet node deactivate #{h}.#{domain}"
				puts "Running 'puppet node deactivate' for: #{h}..."
				`vagrant ssh puppet -c 'sudo #{cmd}'`
			end

			# unsubscribe rhel machines on destroy
			if destroy.is_a?(TrueClass) or (destroy.is_a?(Array) and destroy.include?(h))
				if vm.vm.box.start_with? 'rhel-' or vm.vm.box.start_with? 'atomic-rhel-'
					# check these to know if we registered!
					if username != '' and password != ''
						# ssh into itself and unregister
						cmd = 'subscription-manager unregister'
						puts "Running '#{cmd}' on: #{h}..."
						puts `vagrant ssh #{h} -c 'sudo #{cmd}'`
					end
				end
			end

			#
			#	shell
			#
			if not File.exists?(fv)	# only run these actions once!
				if provision.is_a?(TrueClass) or (provision.is_a?(Array) and provision.include?(h))
					File.open(fv, 'w') {}	# touch
				end

				# subscribe rhel machines
				if vm.vm.box.start_with? 'rhel-' or vm.vm.box.start_with? 'atomic-rhel-'
					if username != '' and password != ''
						vm.vm.provision 'shell', inline: "subscription-manager register --username=#{username} --password=#{password}"
					end

					# attach correct pools
					if vm_poolid.is_a?(Array)
						vm_poolid.each do |j|
							vm.vm.provision 'shell', inline: "subscription-manager attach --pool=#{j}"
						end

					elsif vm_poolid
						# auto attach if value is true
						vm.vm.provision 'shell', inline: 'subscription-manager attach --auto'
					end

					# enable particular repositories
					if vm_repos.is_a?(Array)
						vm_repos.each do |j|
							# if you start the repo name with a #
							# then it will be disabled... this is
							# useful to kill repos from a poolid.
							if j.start_with? '#'
								vm.vm.provision 'shell', inline: "subscription-manager repos --disable=#{j[1,j.length]}"
							else
								vm.vm.provision 'shell', inline: "subscription-manager repos --enable=#{j}"
							end
						end

					elsif vm_repos
						# this enables all repositories
						# NOTE: probably a strong edge
						vm.vm.provision 'shell', inline: "for i in `subscription-manager repos --list | grep '^Repo ID:' | awk '{print $3}'`; do subscription-manager repos --enable=$i; done"
					end

				end

				# update the hosts
				if vm_update
					vm.vm.provision 'shell', inline: 'yum -y update'
				end

				# atomic hosts lack the docker group which the
				# docker provisioner expects... workaround this
				if vm_docker
					vm.vm.provision 'shell', inline: 'getent group docker || groupadd --system docker'
				end
			end

			# TODO: remove this chunk?
			# ensure the $namespace module is present for provisioning...
			if (puppet and vm_puppet and h == 'puppet') and (provision.is_a?(TrueClass) or (provision.is_a?(Array) and provision.include?('puppet')))
				cwd = `pwd`
				mod = File.join(projectdir, 'puppet', 'modules')
				#`cd #{mod} && make MODULENAME=#{namespace} #{namespace} &> /dev/null; cd #{cwd}`
			end

			#
			#	docker (in vm)
			#
			# TODO: do we need to start the docker daemon first ?
			# NOTE: this docker load from file isn't needed as much
			# because vagrant-builder can now docker pull directly:
			# done in git: b6bd325f3a23b25441050afdd8433457278819fd
			if vm_docker.is_a?(Array)
				vm.vm.provision 'shell', inline: 'systemctl start docker'
				vm_docker.each do |j|
					# if there is a docker image already present, use that
					# otherwise pull one down...
					vm.vm.provision 'shell', inline: "if [ -e '/root/docker/#{j[:name]}.docker' ]; then docker load --input='/root/docker/#{j[:name]}.docker'; else docker pull '#{j[:name]}'; fi"
				end

			elsif vm_docker
				vm.vm.provision 'shell', inline: 'systemctl start docker'
				# pull in all the docker images that the vm has
				vm.vm.provision 'shell', inline: 'for i in /root/docker/*.docker; do [ -e "$i" ] || continue; docker load --input="$i"; done'
			end

			# TODO: should we run the docker pull with vagrant too?
			if vm_docker.is_a?(Array)
				vm.vm.provision :docker, images: vm_docker.map { |y| y.fetch(:name, nil)}.compact
			end

			# docker build
			if vm_docker	# for any value that is not false
				Dir.glob(docker_basedir+'*').each do |dd|
					if not File.directory?(dd)
						next	# invalid, skip
					end
					# full Dockerfile path
					df = File.join(dd, 'Dockerfile')
					if not(File.exists?(df)) or File.directory?(df)
						next	# invalid, skip
					end
					# app directory name containing Dockerfile
					b = File.basename(dd)
					vm.vm.provision :docker do |d|
						d.build_image "#{synced_folder}/docker/#{b}", args: "-t #{b}"
						#d.run "#{b}"	# use kube instead
					end
				end
			end

			#
			#	kubernetes
			#
			# NOTE: one could rewrite this kubernetes portion as a
			# vagrant plugin, but until this idea is more baked, i
			# think this is a perfectly fine place to hack this in

			# if we have a master host and if i am an allowed host
			if kubernetes_master and kubernetes_hosts.include?(h)

				# TODO: add support for non yum-based distros
				vm.vm.provision 'shell', inline: 'which kubectl || yum install -y kubernetes'

				# templates are in the kubernetes_templates dir
				ktemplates_host = ((kubernetes_master == h) ? 'master' : 'minion')
				ktemplates = {}
				Dir.glob(File.join(kubernetes_templates, "#{ktemplates_host}/")+'*').each do |ktemplate|
					kbdir = File.basename(ktemplate)
					ktemplates[kbdir] = open(ktemplate, 'r') {|f| f.read}
				end

				kubernetes_ostruct = OpenStruct.new({
					:master => kubernetes_master,
					:hosts => kubernetes_hosts,
					:diff => (kubernetes_hosts - [kubernetes_master]),
					:me => h,
				})

				ktemplates.each do |kpath, template|
					if kpath.end_with?('.erb')
						kpath = File.join('/etc/kubernetes/', kpath[0, kpath.length-'.erb'.length])
						value = ERB.new(template).result(kubernetes_ostruct.instance_eval {binding})
						b64 = Base64.encode64(value)
					else
						# copy data, don't template it!
						kpath = File.join('/etc/kubernetes/', kpath)
						value = template	# str
						b64 = Base64.encode64(value)
					end
					# FIXME: add a diff check so this doesn't clobber unnecessarily
					vm.vm.provision 'shell' do |s|
						s.inline = "echo '#{b64}' | base64 -d > '#{kpath}'"
					end
				end

				# systemctl changes for kubernetes
				if kubernetes_master == h	# master
					services = ['etcd', 'kube-apiserver', 'kube-controller-manager', 'kube-scheduler']
				else
					# a dirty auth hack that scollier wants
					vm.vm.provision 'shell', inline: 'echo "{}" > /var/lib/kubelet/auth'
					services = ['kube-proxy', 'kubelet']
				end
				services.each do |service|
					# a restart causes a start if stopped :)
					vm.vm.provision 'shell', inline: "systemctl restart #{service}"
					vm.vm.provision 'shell', inline: "systemctl enable #{service}"
				end

				# scollier and purpleidea decided against
				# templating the kubernetes json files...
				# run from an application/ dir instead :)
				if kubernetes_master == h	# master
					kubernetes_json.each do |jsonfile|
						roll = false
						if jsonfile.is_a?(Hash)
							roll = jsonfile.fetch('roll', false)
							roll = (['true', 'yes'].include?(roll.downcase) ? true : false)
							jsonfile = jsonfile.fetch('file', '')

							if jsonfile == ''
								$stderr.puts "Warning: Kubernetes application list element is an invalid hash!"
								next
							end
						end

						if not File.exists?(File.join(kubernetes_basedir, jsonfile))
							$stderr.puts "Warning: Kubernetes .json file: '#{jsonfile}' doesn't exist!"
							next
						end

						# get path to json file inside vm...
						jsonfile = File.join("#{synced_folder}/kubernetes/applications/", jsonfile)
						$script = <<-SCRIPT.unindent
							# the sed strips off the quotes from the python string output
							# unescaped sed is: sed 's/\(^"\|"$\)//g'
							kind="`cat '#{jsonfile}' | python -c 'import json,sys;sys.stdout.write(json.dumps(json.load(sys.stdin)["kind"]))' | sed 's/\\(^"\\|"$\\)//g'`"
							kid="`cat '#{jsonfile}' | python -c 'import json,sys;sys.stdout.write(json.dumps(json.load(sys.stdin)["id"]))' | sed 's/\\(^"\\|"$\\)//g'`"
							roll='false'
							case "$kind" in
							'ReplicationController')
								# remap to command arg name
								kind='replicationControllers'
								roll='#{roll}'
							;;
							'Service')
							;;
							'Pod')
							;;
							*)
							echo "Kubernetes kind: '$kind' not yet supported in Oh-My-Vagrant."
							;;
							esac
							echo Running kubernetes...
							if [ "`kubectl get $kind | tail -n +2 | awk '{print $1}'`" = "$kid" ]; then
								# exists...
								# TODO: can we roll like this?
								if [ "$roll" = 'true' ]; then
									echo Running kubernetes: rollingupdate...
									kubectl rollingupdate "$kid" -f '#{jsonfile}'
								else
									# update from existing json file...
									echo Running kubernetes: update...
									kubectl update -f '#{jsonfile}'
								fi
							else
								# new
								echo Running kubernetes: create...
								kubectl create -f '#{jsonfile}'
							fi
						SCRIPT
						vm.vm.provision 'shell', inline: $script
					end
				end
			end

			#
			#	puppet agent - run on puppet client to set it up
			#
			if puppet and vm_puppet and h != 'puppet'
				vm.vm.provision :puppet_server do |p|
					#p.puppet_node = "#{h}"	# redundant
					#p.puppet_server = "puppet.#{domain}"
					p.puppet_server = 'puppet'
					#p.options = '--verbose --debug'
					p.options = '--test'	# see the output

					# extract a list of puppet classes...
					dynamic = {}
					if vm_classes.is_a?(Array)
						puppet_classes = vm_classes

					elsif vm_classes.is_a?(Hash)
						# here we act as a cheap enc...
						puppet_classes = vm_classes.keys
						puppet_classes.each do |name|
							params = vm_classes.fetch(name, {})
							if params.is_a?(Hash)
								keys = []
								params.keys.sort.each do |key|
									dynamic["vagrant_puppet_classes_values_#{name}_#{key}"] = params[key]
									keys.push(key)
								end
								if keys.length > 0
									dynamic["vagrant_puppet_classes_params_#{name}"] = keys.join(',')
								end
							end
						end
					else
						puppet_classes = []
					end

					facter = {
						'vagrant' => '1',
						'vagrant_vip' => vip_ip,
						'vagrant_vip_fqdn' => "#{vip_hostname}.#{domain}",
						'vagrant_puppet_classes' => puppet_classes.join(','),
					}
					facter = facter.merge(dynamic)
					p.facter = facter
				end

			#
			#	puppet apply - run on puppet master to set it up
			#
			elsif puppet and vm_puppet and h == 'puppet'
				vm.vm.provision :puppet do |p|
					p.module_path = folder+'puppet/modules'
					p.manifests_path = folder+'puppet/manifests'
					p.manifest_file = 'site.pp'
					p.hiera_config_path = folder+'puppet/hiera.yaml'
					# custom fact
					p.facter = {
						'vagrant' => '1',
						'vagrant_allow' => vms.each{|z| z[:ip] },
						'vagrant_puppet_folder' => folder,
					}
					p.synced_folder_type = sync
				end

				# the puppetlabs RPM mistakenly starts the
				# service on installation/upgrade. this is a
				# bug, and it causes our server to break,
				# because we want puppet to run with our
				# environment variables, which it doesn't when
				# started by itself! puppetlabs, fix your RPM
				# already!
				# https://tickets.puppetlabs.com/browse/CPR-128
				vm.vm.provision 'shell', inline: 'service puppet stop'
			end

			#
			#	ansible
			#
			if ansible_groups != {}
				vm_playbook.each do |pb|
					vm.vm.provision 'ansible' do |ansible|
						ansible.playbook = folder+'ansible/modules/'+pb
						# reverse the mapping, so that the groups enumerate their hosts
						ansible.groups = Hash[ansible_groups.values.flatten.uniq.map { |i| [i, ansible_groups.select{|_,v| not(v.nil?) and v.include?(i) }.keys ]}]
						ansible.host_key_checking = false
					end
				end
			end

			vm.vm.provider :libvirt do |libvirt|
				# add additional disks to the os
				#(1..disks).each do |j|	# if disks is 0, this passes :)
				#	#print "disk: #{j}"
				#	libvirt.storage :file,
				#		#:path => '',		# auto!
				#		#:device => 'vdb',	# auto!
				#		#:size => '10G',	# auto!
				#		:type => 'qcow2'
				#
				#end

				# make puppet server a bit fatter by default :P
				if puppet and vm_puppet and h == 'puppet'
					libvirt.cpus = 2
					libvirt.memory = 1024
				end
			end
			vm.vm.provider :virtualbox do |vb|
				# make puppet server a bit fatter by default :P
				if puppet and vm_puppet and h == 'puppet'
					vb.customize ["modifyvm", :id, "--cpus", "2"]
					vb.customize ["modifyvm", :id, "--memory", "1024"]
				end
			end
		end
	end

	#
	#	libvirt
	#
	config.vm.provider :libvirt do |libvirt|
		libvirt.driver = 'kvm'	# needed for kvm performance benefits !
		# leave out to connect directly with qemu:///system
		#libvirt.host = 'localhost'
		libvirt.connect_via_ssh = false
		libvirt.username = 'root'
		libvirt.storage_pool_name = 'default'
		#libvirt.default_network = 'default'	# XXX: this does nothing
		libvirt.default_prefix = "#{namespace}"	# set a prefix for your vm's...
	end

	#
	#	virtualbox
	#
	config.vm.provider :virtualbox do |vb|
		# ref params: http://www.virtualbox.org/manual/ch08.html#vboxmanage-modifyvm
		vb.customize ["modifyvm", :id, "--hwvirtex", "on"]	# needed for kvm performance benefits !
		# TODO: Find corresponding params in Virtual Box
		# libvirt.connect_via_ssh = false
		# libvirt.username = 'root'
		# libvirt.storage_pool_name = 'default'
		# #libvirt.default_network = 'default'	# XXX: this does nothing
		# libvirt.default_prefix = "#{namespace}"	# set a prefix for your vm's...
	end

end

